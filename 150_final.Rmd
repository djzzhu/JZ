---
title: "HEDONIC HOME PRICE PREDICTION FOR SAN FRANCISCO"
author: "Jiazheng Zhu, Hanyong Xu"
date: "10/19/2019"
output:
  html_document#:
    theme: united
    toc: TRUE
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	message = FALSE,
	warning = FALSE,
	cache = TRUE,
	results = TRUE
)
options(scipen=99)
```

# 1.1 Introduction

Zillow has realized that its housing market predictions aren’t as accurate as they could be because they do not factor in enough local intelligence. Our group has been selected to build a better predictive model of home prices for San Francisco. 

As far as this project, we will bring geospatial analysis and machine learning techniques together to generate our model. The goal of geospatial prediction is to borrow the experience from one place and test the extent to which that experience generalizes to another place. The machine learning process will be divided into four steps. Below is a brief discussion on each step of the process.

- Data wrangling: The first step of the process is to gather and compile the appropriate data, often from multiple disparate sources, into one consistent dataset.This means the analyst has to understand the nature of the data, how it was collected and how to massage the raw data into useful predictive features.

- Exploratory analysis: Thoughtful exploratory analyis often leads to more useful predictive models. Additionally, more reasonable exploratory research will make the analysis more interpretable for non-technical clients.

- Feature engineering: Feature engineering is the vital point to differ a great model from a good one. Features are the variables used to predict the outcome of interest - in this case home prices. The more the analyst can convert data into useful features, the better her model will perform. There are two key considerations for doing this well. 

- Feature selection: In a geospatial prediction project, it is possible to contain hundreds of features in the dataset. It is much wiser to select useful features in a model instead of implementing all of them. The feature selection process is the process of whittling down all the possible features into a concise and parsimonious set that optimizes for accuracy and generalizability.

- Model estimation and validation: In this project, we will develope a a home price prediction algorithm by using linear regression models. To aviod bias and inaccuracy, we will perform plenty of methods to validate a machine learning model for accuracy and generalizability in the following report.

# 2. Data
## 2.1 Data Collection and Wrangling

Our first geospatial machine learning model will be trained on home price data from San Francisco - one of the largest cities in United States featuring some of the nation’s best travel destinations and a very robust technology sector. In this section, we will create a mapTheme, read the data, re-project the data to State Plane (2227), a coordinate system encoded as feet. 

```{r message=FALSE}
library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(corrplot)
library(viridis)
library(stargazer)
library(tigris)
library(leaflet)
library(tidycensus)
library(tidyverse)
library(MASS)
library(RColorBrewer)
```

```{r}
load(file="150.RData")
```

```{r}
mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=1)
  )
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=1),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}

palette5 <- c("#253494", "#2c7fb8", "#41b6c4", "#a1dab4", "#ffffcc")
palette5 <- c("#ffffcc","#a1dab4", "#41b6c4", "#2c7fb8", "#253494")
```

```{r}
#Two helper functions are included, `qBr` and `q5`
qBr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                          c(.01,.2,.4,.6,.8), na.rm=T)
  }
}

q5 <- function(variable) {as.factor(ntile(variable, 5))}


nn_function <- function(measureFrom,measureTo,k) {
  measureFrom_Matrix <-
    as.matrix(measureFrom)
  measureTo_Matrix <-
    as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
    output <-
      as.data.frame(nn) %>%
      rownames_to_column(var = "thisPoint") %>%
      gather(points, point_distance, V1:ncol(.)) %>%
      arrange(as.numeric(thisPoint)) %>%
      group_by(thisPoint) %>%
      summarize(pointDistance = mean(point_distance)) %>%
      arrange(as.numeric(thisPoint)) %>% 
      dplyr::select(-thisPoint) %>%
      pull()
  
  return(output)  
}
```

```{r include=FALSE}
sf_county <- 
  st_read("C:/Users/HanyongXu/Documents/Me/grad/fall_2019/MUSA507/Project1/Musa_Mid/sf_county.shp") %>% 
  st_transform(2227)
```
### 2.1.1 Dependent Variable: San Francisco Home Price Data

Next, we loaded one shapefile `sp_original` which has 10131 house sale data points from 2012 to 2015. A map of the SF house price is presented below using this data set.

```{r eval=FALSE, include=FALSE}
#Plot the general San Francisco data 
ggplot() + geom_sf(data = nhood) + mapTheme()
```

```{r}
ggplot()+
  geom_sf(data=sf_county)+
  geom_sf(data=sp_original,aes(colour = q5(SalePrice)), show.legend = "point", size = .75)+
  scale_colour_manual(values = palette5,
                   labels=qBr(sp,"SalePrice"),
                   name="Quintile\nBreaks") +
  labs(title="Dependent Variable: Sale Price, SF") +
  mapTheme()
```

### 2.1.2 Independent Variables

The data set above also includes various characteristics for San Francisco homes that are useful to our model construction, including: 

- sale price
- built year (house age is converted using `houseAge` = 2019 - `BuiltYear`)
- number of rooms, bedrooms, bathrooms (3 signigicant outliers are mannually changed according to the neighboring room numbers)
- property and lot area (2 signigicant outliers are mannually changed according to the median value)
- property class
- sale date (date is seperated into year, month, and day)
- number of stories (4 signigicant outliers are mannually changed according to Google Street Map)

Census tract data `tracts` is downloaded using `tigirs`, which is a powerful package to access data in United States Census Bureau. A couple of tracts are removed from the original dataset because of the scope. Both data is reprojected to the appropriate State Plane.

```{r eval=FALSE, include=FALSE}
#load house price data from Ken
sp_original <- 
  st_read("C:/Users/HanyongXu/Documents/Me/grad/fall_2019/MUSA507/Project1/midtermData_studentVersion/midterm_data_sf_studentVersion.shp") %>% 
  st_transform(2227)
```

```{r eval=FALSE, include=FALSE}
options(tigris_class = "sf")
tracts <- 
  tracts("06","075") %>% 
  st_transform(2227) %>% 
  filter(NAME!=9804.01&NAME!=9901&NAME!=179.02&NAME!=601)

bsmp <- st_union(tracts)
```

Various data sets are downloaded from the [San Francisco Open Data](https://datasf.org/opendata/), which consists of the city's economic, environmental, social, and many other aspects data. From this website, we have downloaded:

- road network (distance from house to road is calculated)
- transportation stops (both the number of transit stations within )
- [park](https://data.sfgov.org/api/geospatial/gtr9-ntp6?method=export&format=GeoJSON)
- [school](https://data.sfgov.org/api/geospatial/tpp3-epx2?method=export&format=GeoJSON)
- [slope](https://data.sfgov.org/api/geospatial/rnbg-2qxw?method=export&format=GeoJSON)
- commercial area and open space (plazas)
- crime
- tree canopy
- water bodies
- hazards
- [neighborhoods](https://data.sfgov.org/api/geospatial/5gzd-g9ns?method=export&format=GeoJSON)
- height limits in zones
- zoning
- [technology companies, arts/entertainment/recreation, real estate and leasing services, financial services, private health services](https://data.sfgov.org/Economy-and-Community/Registered-Business-Locations-San-Francisco/g8m3-pdis)

Census data is downloaded through the `tidycensus` package, which allows access to both the decennial and the ACS census data. Here, we have chosen the ACS 5 year estimates in 2017 at the census tract level to work on. Both the ACS profile and the detailed table are used. Variables selected from the census include:

- percent of residents 25 years or older holding a bachelor's degree or higher
- percent of 18 years and younger
- percent of 65 years and older
- percent of foreign born residents
- percent of residence in the same house for 1 year and over
- percent of vacant houses
- percent of detached houses
- percent of renters
- percent of households with 2 vehicles
- percent of households with 3 or more vehicles
- median home value
- median home rent
- median household income
- total population
- white population
- black population
- asian population
- hispanic population

In the end, the locations of the precidio, twin peaks, the golden gate and the University of San Francisco are mannually combined into one data set.

```{r eval=FALSE, include=FALSE}
#read(roads) wjz
options(tigris_class = "sf")
Roads <- 
  roads("06","075")  %>% 
  st_transform(st_crs(bsmp)) %>%
  st_intersection(.,st_union(bsmp))
```

```{r eval=FALSE, include=FALSE}
ggplot() +
  geom_sf(data=bsmp) +
  geom_sf(data=Roads) +
  labs(title="Our basemap")
```
```{r eval=FALSE, include=FALSE}
na.omit(sp,ConstType)
reg <- lm(SalePrice ~ PropClassC + SaleDate + LotArea + PropArea + BuiltYear + Stories +   
            Units + Rooms + Beds + Baths + SaleYr, 
          data = sp) 

summary(reg)
```

```{r eval=FALSE, include=FALSE}
#read(transport) jz
transport <- read.csv("D:/GIS/mid term/midtermData_studentVersion/Muni_Stops.csv")
transport.sf <- 
   transport %>% 
   st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs = 4326, agr = "constant") %>%
   st_transform(2227)
```

```{r eval=FALSE, include=FALSE}
#download the data from ACS 5 yr 2017

options(tigris_use_cache = TRUE)
#census_api_key("da0e988e55392485b75bc3cc569605b96273839a", install = TRUE)
#readRenviron("~/.Renviron")
#Sys.getenv("CENSUS_API_KEY")

#load acs profile and detailed tables
acs17profile <- load_variables(2017, "acs5/profile", cache = FALSE)
acs17 <- load_variables(2017, "acs5", cache = FALSE)

#below are the data from ACS profile
tracts17_p <- 
  get_acs(geography = "tract", variables = c("DP02_0067P", "DP05_0019P","DP05_0024P", 
                                             "DP02_0101P", "DP02_0079P", "DP04_0003P", 
                                             "DP04_0007P", "DP04_0047P", "DP04_0060P",
                                             "DP04_0061P", "DP04_0089", "DP04_0134"), 
          year = 2017, state=06, county=075, geometry=T) %>%
  st_transform(2227)  %>%
  dplyr::select(variable, estimate, GEOID) %>%
  spread(variable, estimate) %>%
  rename(pcbachedegree = DP02_0067P,
         pcunder18 = DP05_0019P,
         pcabove65 = DP05_0024P,
         pcforeighborn = DP02_0101P,
         pcsamehouse = DP02_0079P,
         pcvacant = DP04_0003P,
         pcdetached = DP04_0007P,
         pcrenter = DP04_0047P,
         pc2vehicles = DP04_0060P,
         pc3ormorevehicles = DP04_0061P,
         medianhomevalue = DP04_0089,
         medianrent = DP04_0134)

#below are the data from ACS detailed table
tracts17 <- 
  get_acs(geography = "tract", variables = c("B02001_002","B02001_003","B02001_005","B03002_012",
                                             "B19013_001", "B01003_001"), 
          year = 2017, state=06, county=075, geometry=T) %>%
  st_transform(2227)  %>%
  dplyr::select(variable, estimate, GEOID) %>%
  spread(variable, estimate) %>%
  rename(white = B02001_002,
         black = B02001_003,
         asian = B02001_005,
         hispanic = B03002_012,
         med_hhincome = B19013_001,
         totalpop = B01003_001) %>%
  mutate(pcWhite = white / totalpop,
         pcBlack = black / totalpop,
         pcAsian = asian / totalpop,
         pcHispanic = hispanic / totalpop)

tracts17 = st_drop_geometry(tracts17)
tracts17tot <- merge(tracts17, tracts17_p, by = "GEOID")
```

```{r eval=FALSE, include=FALSE}
#read(park) JZ

park <- st_read("https://data.sfgov.org/api/geospatial/gtr9-ntp6?method=export&format=GeoJSON")
park$latitude <- as.numeric(as.character(park$latitude)) 
park$longitude <- as.numeric(as.character(park$longitude))
park.sf <-
  park %>%
    st_as_sf(coords = c("latitude", "longitude"), crs = 4326, agr = "constant") %>%
    st_transform(2227)

#add parkfilter
parkfilter <-
  park %>% 
  filter(latitude > 37.7, longitude < -122)

parkfilter.sf <-
  park %>% 
  filter(latitude > 37.7, longitude < -122)

parkfilter_d.sf <-
  parkfilter %>%
    dplyr::select(latitude, longitude) %>%
    na.omit() %>%
    st_as_sf(coords = c("Longitute", "Latitude"), crs = 4326, agr = "constant") %>%
    st_transform(2227) %>%
    distinct()

sp_original$park.Buffer =
    sp_original%>%
    st_buffer(2640) %>%
    aggregate(mutate(parkfilter_d.sf, park.Buffer = 1),., sum) %>%
    pull(park.Buffer)

park_cent <- st_centroid(parkfilter_d.sf)

sp_original$park_nn1 =
    nn_function(st_coordinates(sp_original), st_coordinates(park_cent), 1)

sp_original$park_nn2 =
    nn_function(st_coordinates(sp_original), st_coordinates(park_cent), 2)

sp_original$park_nn3 =
    nn_function(st_coordinates(sp_original), st_coordinates(park_cent), 3)

#park larger than 50 acres
parkfilter.sf$acres <- as.numeric(as.character(parkfilter.sf$acres)) 

park50.sf <- 
  parkfilter.sf %>% 
  select(acres) %>%
  filter(acres > 50)
park50.sf <- park50.sf[,2]
```

```{r eval=FALSE, include=FALSE}
#(school district) JZ
school_districts <- 
  st_read("https://data.sfgov.org/api/geospatial/tpp3-epx2?method=export&format=GeoJSON") %>% 
  st_transform(2227) 

#school distance nn
school_district.sf <-
  school_districts %>%
    dplyr::select(geometry) %>%
    na.omit() %>%
    st_transform(2227) %>%
    distinct()

sp_original$school.buffer =
    sp_original%>%
    st_buffer(2640) %>%
    aggregate(mutate(school_district.sf, school.Buffer = 1),., sum) %>%
    pull(school.Buffer)

sp_original$school_nn1 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 1)

sp_original$school_nn2 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 2)

sp_original$school_nn3 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 3)

sp_original$school_nn4 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 4)

sp_original$school_nn5 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 5)

school_districtsprivate <- subset(school_districts, ccsf_entity == "Private")

#private school distance
school_districtsprivate.sf <-
  school_districtsprivate %>%
    dplyr::select(geometry) %>%
    na.omit() %>%
    st_transform(2227) %>%
    distinct()

sp_original$school.bufferpr =
    sp_original%>%
    st_buffer(2640) %>%
    aggregate(mutate(school_districtsprivate.sf, school.bufferpr = 1),., sum) %>%
    pull(school.bufferpr)

sp_original$schoolp_nn1 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 1)

sp_original$schoolp_nn2 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 2)

sp_original$schoolp_nn3 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 3)

sp_original$schoolp_nn4 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 4)

sp_original$schoolp_nn5 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 5)

```


```{r eval=FALSE, include=FALSE}
#slope hy
slope <- 
  st_read("https://data.sfgov.org/api/geospatial/rnbg-2qxw?method=export&format=GeoJSON") %>%
  st_set_crs(4326) %>%
  st_transform(2227)
```

```{r eval=FALSE, include=FALSE}
#commercial area and open space, JZ
#distance to nearest commercial area and open space&rec
distance_op_co <- 
  st_read("D:/GIS/mid term/midtermData_studentVersion/midtermData_studentVersion/Land Use/Distance_Op_Co_et.shp") %>%
  st_transform(2227) %>% 
  filter(SalePrice!=0)

distance_op_co.sf <- 
  distance_op_co %>% 
  st_transform(2227)

sp_original <-
  sp_original %>%
    st_join(dplyr::select(distance_op_co,Distance,Distance_1))

sp_original %>%
  dplyr::select(Distance, SalePrice) %>%
  st_set_geometry(NULL) %>%
  gather(Variable, Value, -SalePrice) %>%
    ggplot(aes(Value, SalePrice)) +
      geom_point() +
      geom_smooth(method = "lm", se=F, colour = "#25CB10")
```

```{r eval=FALSE, include=FALSE}
#crime wjz
crime.sf <- 
  st_read("C:/Users/Jingzong/Google Drive/MUSA/Mid_Project/HomePricePrediction/CRIME.geojson")%>% 
  st_transform(2227)
```
```{r eval=FALSE, include=FALSE}
ggplot() +
  geom_sf(data=bsmp)+
  scale_colour_manual(values = palette5,
                   labels=qBr(sp,"SalePrice"),
                   name="Quintile\nBreaks") +
  geom_sf(data = crime.sf, colour = "black", size = .1) +
  geom_sf(data=sp，aes(colour = q5(SalePrice)), show.legend = "point", size = .1)+
  labs(title = "2012-2015 aggravated assaults&roberry, SF") +
  mapTheme()
```

```{r eval=FALSE, include=FALSE}
#crime ready
sp_original1$crime_nn1 =
    nn_function(st_coordinates(sp_original1), st_coordinates(crime.sf), 1)
sp_original1$crime_nn2 =
    nn_function(st_coordinates(sp_original1), st_coordinates(crime.sf), 2)
sp_original1$crime_nn3 =
    nn_function(st_coordinates(sp_original1), st_coordinates(crime.sf), 3)
sp_original1$crime_nn4 =
    nn_function(st_coordinates(sp_original1), st_coordinates(crime.sf), 4)
sp_original1$crime_nn5 =
    nn_function(st_coordinates(sp_original1), st_coordinates(crime.sf), 5)
```

```{r eval=FALSE, include=FALSE}
####elevation
elev_index <- 
  sp_original %>% 
  st_nearest_feature(dplyr::select(slope, elevation))
st_geometry(slope) <- NULL
```

```{r eval=FALSE, include=FALSE}
sp_elev <- data.frame()
i <- 1
while (i <= length(elev_index)) {
    i_elev <- as.data.frame(slope[elev_index[i],1])
    sp_elev <- rbind(sp_elev, i_elev)
  i <- i+1
}
```
```{r eval=FALSE, include=FALSE}
sp_original1$elevation <-  sp_elev$`slope[elev_index[i], 1]`
sp_original1$elevation <- as.numeric(levels(sp_original1$elevation))[sp_original1$elevation]
```

```{r eval=FALSE, include=FALSE}
as.data.frame(sp_original)  %>% 
filter(SalePrice!=0) %>% 
ggplot(aes(elevation, SalePrice)) +
     geom_point() +
      geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     theme(axis.text.x = element_blank()) +
     plotTheme()
```

```{r eval=FALSE, include=FALSE}
reg_ele <- 
  lm(SalePrice ~ elevation, data=sp_original)

summary(reg_ele)
```

```{r eval=FALSE, include=FALSE}
###slope slope wjz
slope <- read.csv("C:/Users/Jingzong/Google Drive/MUSA/Mid_Project/GIS/slope.txt")
```

```{r eval=FALSE, include=FALSE}
sp_original1 <- merge(sp_original1,slope,by.x=c("X", "Y"), by.y=c("X", "Y"))
colnames(sp_original1)[colnames(sp_original1)=="RASTERVALU"] <- "slope"
```

```{r eval=FALSE, include=FALSE}
reg_slop <- 
  lm(SalePrice ~ slope, data=sp_original)
summary(reg_slop)
```
```{r eval=FALSE, include=FALSE}
as.data.frame(sp_original)  %>% 
filter(SalePrice!=0) %>% 
ggplot(aes(slope, SalePrice)) +
     geom_point() +
      geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     theme(axis.text.x = element_blank()) +
     plotTheme()
```

```{r eval=FALSE, include=FALSE}
#TREE CANOPY BY JZ
tree_canopy1 <- 
  st_read("D:/GIS/mid term/midtermData_studentVersion/midtermData_studentVersion/midterm_data_sf_tree_total2.shp") %>%
  st_transform(2227)

tree_canopy1 <- 
  as.data.frame(tree_canopy1) %>% 
  group_by(ParcelID) %>% 
  summarize(totaltree = sum(t_sqft))

sp_original <- merge(as.data.frame(tree_canopy1),as.data.frame(sp_original),by="ParcelID")

```

```{r eval=FALSE, include=FALSE}
#water JZ
water <- 
  st_read("D:/GIS/mid term/midtermData_studentVersion/Musa_Mid/Water bodies/water_distance.shp") %>% 
  st_transform(2227)

water <-
  water %>% 
  select(ParcelID,water)

water$geometry = NULL

sp_original1 <- merge(as.data.frame(water),as.data.frame(sp_original1),by="ParcelID")

sp_original1 %>%
  dplyr::select(water, SalePrice) %>%
  gather(Variable, Value, -SalePrice) %>%
    ggplot(aes(Value, SalePrice)) +
      geom_point() +
      geom_smooth(method = "lm", se=F, colour = "#25CB10") +
      facet_wrap(~Variable)
```
 

```{r eval=FALSE, include=FALSE}
#HAZARDS JZ
hazards <- 
  st_read("D:/GIS/mid term/midtermData_studentVersion/Musa_Mid/San Francisco Seismic Hazard Zones/Join_Outputdistance2.shp") %>% 
  st_transform(2227)

hazards <-
  hazards %>% 
  select(ParcelID,Distance_N)

hazards$geometry = NULL

sp_original1 <- merge(as.data.frame(hazards),as.data.frame(sp_original1),by="ParcelID")


sp_original1 %>%
  dplyr::select(Distance.y, SalePrice) %>%
  gather(Variable, Value, -SalePrice) %>%
    ggplot(aes(Value, SalePrice)) +
      geom_point() +
      geom_smooth(method = "lm", se=F, colour = "#25CB10") +
      facet_wrap(~Variable)
```

```{r eval=FALSE, include=FALSE}
#census data cleaning by hy, cleaned data in dataframe sp_original1
#spatial join census to the home price
tracts17tot.sf <- st_as_sf(tracts17tot)
sp_original1 <- st_join(sp_original, tracts17tot.sf)

#transform builtyear from factors to numbers
sp_original1$BuiltYear <- as.numeric(levels(sp_original1$BuiltYear))[sp_original1$BuiltYear]

#transform builtyear to age
sp_original1 <- sp_original1 %>%
  mutate(houseAge = 2019 - BuiltYear)

#data cleaning: assign correct stories of the houses
sp_original1$Stories[sp_original1$Stories==829] <- 2 #parcel id 1242021
sp_original1$Stories[sp_original1$Stories==742] <- 1 #7016023
sp_original1$Stories[sp_original1$Stories==432] <- 2 #1153051
sp_original1$Stories[sp_original1$Stories==10] <- 2 #6655075
#data cleaning: Rooms
sp_original1$Rooms[sp_original1$Rooms==1353] <- 6 #4148041 used median as substitute
#data cleaning: Baths
sp_original1$Baths[sp_original1$Baths==25] <- 2 #7106045A,6481018 used median as substitute
#data cleaning: PropArea
sp_original1$PropArea[sp_original1$PropArea==24308] <- 2500 #5837002 used max neighbor area as substitute
sp_original1$PropArea[sp_original1$PropArea==22650] <- 3750 #3621070 
#data cleaning: Beds
sp_original1$Beds[sp_original1$Beds==20] <- 2 #0552062 used neighbor data as substitute
#rooms = 0 still need to be cleaned
```

```{r eval=FALSE, include=FALSE}
reg.results <- 
  data.frame(Observed = sp$SalePrice,
             Predicted = reg$fitted.values)

ggplot() + 
  geom_point(data=reg.results, aes(Observed, Predicted)) +
  stat_smooth(data=reg.results, aes(Observed, Observed), method = "lm", se = FALSE, size = 1, colour="red") + 
  stat_smooth(data=reg.results, aes(Observed, Predicted), method = "lm", se = FALSE, size = 1, colour="blue") + 
  labs(title="Predicted Sales Price as a function\nof Observed Sales Price",
       subtitle="Perfect prediction in red; Actual prediction in blue") +
  theme(plot.title = element_text(size = 18,colour = "black"))
```

```{r eval=FALSE, include=FALSE}
hist(abs(sp$SalePrice - reg$fitted.values), breaks=50, main="Histrogram of residuals (absolute values)")
```

## 2.2 Exploratory Analysis
### 2.2.1 Summary Statistics

We have categorized our predictors into three types: internal characteristics, amenities/public services, and spatial structue. Their summary statistics are presented below. Note that a few variables are not in the tables below since they are spatial or categorical data, including the neighborhoods and zonings.

```{r}
#stargazer(as.data.frame(sp_original1), nobs = FALSE, type="text", title = "Summary Statistics")

stargazer(as.data.frame(sp_original1)[c("SalePrice", "LotArea", "PropArea", "BuiltYear", "Stories", "Rooms", "Beds", "Baths", "SaleYr", "houseAge")], digits = 2, type = "text", nobs = FALSE, title = "Internal Characteristics")

stargazer(as.data.frame(sp_original1)[c("Distance_N", "water", "Distance.x", "totaltree", "crime_nn1", "crime_nn2", "crime_nn3", "crime_nn4", "crime_nn5", "park.Buffer", "park_nn1", "park_nn2", "park_nn3", "school.buffer", "school_nn1", "school_nn2", "school_nn3", "school_nn4", "school_nn5", "Distance.y", "transit.buffer", "tech_nn3", "tech_nn4", "tech_nn5", "art_nn3", "art_nn4", "art_nn5", "real_est_nn3", "real_est_nn4", "real_est_nn5", "Pr_hlth_nn3", "Pr_hlth_nn4", "Pr_hlth_nn5", "Financ_nn3", "Financ_nn4", "Financ_nn5","ind_nn1", "ind_nn2", "ind_nn3", "min_dis_road", "Distance")], digits = 2, type = "text", nobs = FALSE, title = "Amenities/Public Services")

stargazer(as.data.frame(sp_original1)[c("totalpop", "med_hhincome", "pcWhite", "pcBlack", "pcAsian", "pcHispanic","pcbachedegree", "pcsamehouse", "pcforeighborn", "pcvacant", "pcdetached", "pcrenter", "pc2vehicles", "pc3ormorevehicles", "medianhomevalue", "medianrent", "pcunder18", "pcabove65", "elevation", "slope", "lagPrice")], digits = 2, type = "text", nobs = FALSE, title = "Spatial Structure")
```

### 2.2.2 Correlation between all the variables

A correlation matrix is produced to see the relationship between the continuous variables. The non-continuous variables, including all the census data variables, are dropped. To further simplify the correlation plot, only one variable is kept for each amenity. 

```{r}
sp_original1cor <- 
  filter(sp_original1, holdOut =="0") %>%
  st_set_geometry(NULL) %>%
  dplyr::select(SalePrice, LotArea, PropArea, Stories, Rooms, Beds, Baths, SaleYr, houseAge, Distance_N, water, Distance.x, totaltree, crime_nn5, park_nn3, school_nn5, Distance.y, transit.buffer, tech_nn5,art_nn5,real_est_nn5,Pr_hlth_nn5, Financ_nn5, ind_nn3, min_dis_road, Distance, elevation, lagPrice)

#totalpop, med_hhincome, pcWhite, pcBlack, pcAsian, pcHispanic, pcbachedegree, pcsamehouse, pcforeighborn, pcvacant, pcdetached, pcrenter, pc2vehicles, pc3ormorevehicles, medianhomevalue, medianrent, pcunder18, pcabove65,

M <- cor(sp_original1cor)
corrplot(M, number.cex = .7, tl.srt = 45, method = "color", type = "lower", tl.col = "black", na.label = ".", na.rm = T, tl.cex = 0.7, col = brewer.pal(n = 10, name = "RdYlBu"))
```

A series explorations of correlation between the dependent variable sale price and other predictors are conducted using the scatter plots. Here, four of the most well-fitted scatter plots are presented. `Fianc_nn5` is the distance to 5 Nearest financial services, `ind_nn3` is the distance to 3 nearest attractions, `pcsamehouse` is the percentage of residents lived in the same house for over 1 year, and `tech_nn5` is the distance to 5 nearest tech companies.

```{r}
labeli <- function(variable, value){
  names_li <- list("Financ_nn5"="Distance to 5 Nearest Financial Services", "ind_nn3"="Distance to 3 Nearest Attractions", "pcsamehouse"="Percentage of Residents Lived in the Same House for over 1 Year", "tech_nn5"="Distance to 5 Nearest Tech Companies")
  return(names_li[value])
}

as.data.frame(sp_original1) %>% 
  dplyr::select(SalePrice,  Financ_nn5, ind_nn3, pcsamehouse, tech_nn5) %>%
  filter(SalePrice != 0) %>%
  gather(Variable, Value, -SalePrice) %>% 
  ggplot(aes(Value, SalePrice), labeller = labeli) +
  geom_point() +
  geom_smooth(method = "lm", se=F, colour = "#FA7800") +
  facet_wrap(~Variable, ncol = 2, scales = "free", ) +
  labs(title = "Price as a Function of Four Predictors") +
  plotTheme()
```

### 2.2.3 Spatial Distribution

Three of the independent variables are mapped below. For the first two variables, both the density of the amenity and the distance to the 5 nearest amenities are mapped. For the third variable, elevation, both the point elevation and the countour lines are mapped.

```{r eval=FALSE, include=FALSE}
county <- 
  st_read("https://data.sfgov.org/api/geospatial/s9wg-vcph?method=export&format=GeoJSON") %>%
  st_set_crs(4326) %>%
  st_transform(2227)

countysf <- subset(county, county == "San Francisco")

countysf <- 
  countysf  %>% 
  st_transform(st_crs(nhood)) %>%
  st_intersection(.,nhood)
```

```{r}
real_est1 <- 
  real_est  %>% 
  st_intersection(.,nhood)
```

```{r}
ggplot() + 
  geom_sf(data = sf_county, fill = "grey90") +
  stat_density2d(data = data.frame(st_coordinates(real_est1)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "#fee391", high = "#cc4c02", name = "Density of Real Estates \nand Leasing Services") +
  scale_alpha(range = c(.1, .3), guide = FALSE) +
  geom_sf(data = sp_original1, aes(color = real_est_nn5), show.legend = "point", size = .75) + 
    labs(title="Real Estate and Leasing Services Proximity") +
    scale_colour_viridis(name="House Distance to \n5 Closest Real Estate\nand Leasing Services, ft") +
  mapTheme()
```

```{r}
school_district.sf1 <- 
  school_district.sf  %>% 
  st_transform(st_crs(nhood)) %>%
  st_intersection(.,nhood)
```

```{r}
ggplot() + 
  geom_sf(data = sf_county, fill = "grey90") +
  stat_density2d(data = data.frame(st_coordinates(school_district.sf1)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "#fee391", high = "#cc4c02", name = "Density of Schools") +
  scale_alpha(range = c(0, .2), guide = FALSE) +
  geom_sf(data = sp_original1, aes(color = school_nn5), show.legend = "point", size = .75) + 
    labs(title="School Proximity, SF") +
    scale_colour_viridis(name="House Distance to the\n5 Closest Schools\nft") +
  mapTheme()
```

```{r include=FALSE}
slope1 <- 
  st_read("https://data.sfgov.org/api/geospatial/rnbg-2qxw?method=export&format=GeoJSON") %>%
  st_set_crs(4326) %>%
  st_transform(2227) %>%
  st_intersection(.,nhood)
```
```{r include=FALSE}
slope1$elevation <- as.numeric(levels(slope1$elevation))[slope1$elevation]
slope1 <- slope1[ which(slope1$elevation==-50 | slope1$elevation==50 | slope1$elevation==150| slope1$elevation==250|slope1$elevation==350|slope1$elevation==450|slope1$elevation==550|slope1$elevation==650|slope1$elevation==750|slope1$elevation==850|slope1$elevation==950), ]
```


```{r}
ggplot() + 
  geom_sf(data = sf_county, fill = "grey90") +
  geom_sf(data = slope1, color = "grey70", size = 0.1) +
  geom_sf(data = sp_original1, aes(color = elevation), show.legend = "point", size = .75) + 
    labs(title="Elevation") +
    scale_colour_viridis(name="Elevation, ft") +
  mapTheme()
```

