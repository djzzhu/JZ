---
title: "HEDONIC HOME PRICE PREDICTION FOR SAN FRANCISCO"
author: "Jiazheng Zhu, Hanyong Xu"
date: "10/19/2019"
output:
  html_document#:
    theme: united
    toc: TRUE
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	message = FALSE,
	warning = FALSE,
	cache = TRUE,
	results = TRUE
)
options(scipen=99)
```

# 1.1 Introduction

Zillow has realized that its housing market predictions aren’t as accurate as they could be because they do not factor in enough local intelligence. Our group has been selected to build a better predictive model of home prices for San Francisco. Because there are so many variables and elements to influence home value in San Francisco, it is such a tough task to perform prediction. Even though it may takes substantial times and energy to gather and process raw data, it will still be interesting to take this challenge. 

As far as this project, we will bring geospatial analysis and machine learning techniques together to generate our model. The goal of geospatial prediction is to borrow the experience from one place and test the extent to which that experience generalizes to another place. The machine learning process will be divided into four steps. Below is a brief discussion on each step of the process.

- Data wrangling: The first step of the process is to gather and compile the appropriate data, often from multiple disparate sources, into one consistent dataset.This means the analyst has to understand the nature of the data, how it was collected and how to massage the raw data into useful predictive features.

- Exploratory analysis: Thoughtful exploratory analyis often leads to more useful predictive models. Additionally, more reasonable exploratory research will make the analysis more interpretable for non-technical clients.

- Feature engineering: Feature engineering is the vital point to differ a great model from a good one. Features are the variables used to predict the outcome of interest - in this case home prices. The more the analyst can convert data into useful features, the better her model will perform. There are two key considerations for doing this well. 

- Feature selection: In a geospatial prediction project, it is possible to contain hundreds of features in the dataset. It is much wiser to select useful features in a model instead of implementing all of them. The feature selection process is the process of whittling down all the possible features into a concise and parsimonious set that optimizes for accuracy and generalizability.

- Model estimation and validation: In this project, we will develope a a home price prediction algorithm by using linear regression models. To aviod bias and inaccuracy, we will perform plenty of methods to validate a machine learning model for accuracy and generalizability in the following report.

Generally, the home value model we built can predict approximately 85 percent of the total homes from 2012-2015. The error is about 180,000 dollars. Considering the situation of home prediction difficulty, we think this model is well-performed. 

# 2. Data
## 2.1 Gathering the Data and Data Wrangling

Our first geospatial machine learning model will be trained on home price data from San Francisco - one of the largest cities in United States featuring some of the nation’s best travel destinations and a very robust technology sector. In this section, we will create a mapTheme, read the data, re-project the data to State Plane (2227), a coordinate system encoded as feet. 
```{r message=FALSE}
library(tidyverse)
library(sf)
library(spdep)
library(caret)
library(ckanr)
library(FNN)
library(grid)
library(gridExtra)
library(corrplot)
library(viridis)
library(stargazer)
library(tigris)
library(leaflet)
library(tidycensus)
library(tidyverse)
library(MASS)
library(RColorBrewer)
library(kableExtra)
```

```{r}
load(file="150.RData")
```

```{r}
mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=1)
  )
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=1),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}

palette5 <- c("#253494", "#2c7fb8", "#41b6c4", "#a1dab4", "#ffffcc")
palette5 <- c("#ffffcc","#a1dab4", "#41b6c4", "#2c7fb8", "#253494")
```

```{r}
#Two helper functions are included, `qBr` and `q5`
qBr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                          c(.01,.2,.4,.6,.8), na.rm=T)
  }
}

q5 <- function(variable) {as.factor(ntile(variable, 5))}


nn_function <- function(measureFrom,measureTo,k) {
  measureFrom_Matrix <-
    as.matrix(measureFrom)
  measureTo_Matrix <-
    as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
    output <-
      as.data.frame(nn) %>%
      rownames_to_column(var = "thisPoint") %>%
      gather(points, point_distance, V1:ncol(.)) %>%
      arrange(as.numeric(thisPoint)) %>%
      group_by(thisPoint) %>%
      summarize(pointDistance = mean(point_distance)) %>%
      arrange(as.numeric(thisPoint)) %>% 
      dplyr::select(-thisPoint) %>%
      pull()
  
  return(output)  
}
```

```{r eval=FALSE, include=FALSE}
sf_county <- 
  st_read("C:/Users/Jiazheng Zhu/Downloads/sp_county-20191020T223655Z-001/sp_county/sf_county.shp") %>% 
  st_transform(2227)
```
### 2.1.1 Dependent Variable: San Francisco Home Price Data

Next, we loaded one shapefile `sp_original` which has 10131 house sale data points from 2012 to 2015. A map of the SF house price is presented below using this data set.

![](D:/GIS/mid term/New folder/HJZ.Group.Inc/plot/unnamed-chunk-7-1.png)

```{r eval=FALSE, include=FALSE}
#Plot the general San Francisco data 
ggplot() + geom_sf(data = nhood) + mapTheme()
```

```{r eval=FALSE, include=FALSE}
ggplot()+
  geom_sf(data=sf_county)+
  geom_sf(data=sp_original,aes(colour = q5(SalePrice)), show.legend = "point", size = .75)+
  scale_colour_manual(values = palette5,
                   labels=qBr(sp,"SalePrice"),
                   name="Quintile\nBreaks") +
  labs(title="Dependent Variable: Sale Price, SF") +
  mapTheme()
```

### 2.1.2 Independent Variables

The data set above also includes various characteristics for San Francisco homes that are useful to our model construction, including: 

- sale price
- built year (house age is converted using `houseAge` = 2019 - `BuiltYear`)
- number of rooms, bedrooms, bathrooms (3 signigicant outliers are mannually changed according to the neighboring room numbers)
- property and lot area (2 signigicant outliers are mannually changed according to the median value)
- property class
- sale date (date is seperated into year, month, and day)
- number of stories (4 signigicant outliers are mannually changed according to Google Street Map)

Census tract data `tracts` is downloaded using `tigirs`, which is a powerful package to access data in United States Census Bureau. A couple of tracts are removed from the original dataset because of the scope. Both data is reprojected to the appropriate State Plane.

```{r eval=FALSE, include=FALSE}
#load house price data from Ken
sp_original <- 
  st_read("C:/Users/HanyongXu/Documents/Me/grad/fall_2019/MUSA507/Project1/midtermData_studentVersion/midterm_data_sf_studentVersion.shp") %>% 
  st_transform(2227)
```

```{r eval=FALSE, include=FALSE}
options(tigris_class = "sf")
tracts <- 
  tracts("06","075") %>% 
  st_transform(2227) %>% 
  filter(NAME!=9804.01&NAME!=9901&NAME!=179.02&NAME!=601)

bsmp <- st_union(tracts)
```

Various data sets are downloaded from the [San Francisco Open Data](https://datasf.org/opendata/), which consists of the city's economic, environmental, social, and many other aspects data. From this website, we have downloaded:

- road network (distance from house to road is calculated)
- transportation stops (both the number of transit stations within )
- [park](https://data.sfgov.org/api/geospatial/gtr9-ntp6?method=export&format=GeoJSON)
- [school](https://data.sfgov.org/api/geospatial/tpp3-epx2?method=export&format=GeoJSON)
- [slope](https://data.sfgov.org/api/geospatial/rnbg-2qxw?method=export&format=GeoJSON)
- commercial area and open space (plazas)
- crime
- tree canopy
- water bodies
- hazards
- [neighborhoods](https://data.sfgov.org/api/geospatial/5gzd-g9ns?method=export&format=GeoJSON)
- height limits in zones
- zoning
- [technology companies, arts/entertainment/recreation, real estate and leasing services, financial services, private health services](https://data.sfgov.org/Economy-and-Community/Registered-Business-Locations-San-Francisco/g8m3-pdis)

Census data is downloaded through the `tidycensus` package, which allows access to both the decennial and the ACS census data. Here, we have chosen the ACS 5 year estimates in 2017 at the census tract level to work on. Both the ACS profile and the detailed table are used. Variables selected from the census include:

- percent of residents 25 years or older holding a bachelor's degree or higher
- percent of 18 years and younger
- percent of 65 years and older
- percent of foreign born residents
- percent of residence in the same house for 1 year and over
- percent of vacant houses
- percent of detached houses
- percent of renters
- percent of households with 2 vehicles
- percent of households with 3 or more vehicles
- median home value
- median home rent
- median household income
- total population
- white population
- black population
- asian population
- hispanic population

In the end, the locations of the precidio, twin peaks, the golden gate and the University of San Francisco are mannually combined into one data set.

```{r eval=FALSE, include=FALSE}
#read(roads) wjz
options(tigris_class = "sf")
Roads <- 
  roads("06","075")  %>% 
  st_transform(st_crs(bsmp)) %>%
  st_intersection(.,st_union(bsmp))
```

```{r eval=FALSE, include=FALSE}
ggplot() +
  geom_sf(data=bsmp) +
  geom_sf(data=Roads) +
  labs(title="Our basemap")
```
```{r eval=FALSE, include=FALSE}
na.omit(sp,ConstType)
reg <- lm(SalePrice ~ PropClassC + SaleDate + LotArea + PropArea + BuiltYear + Stories +   
            Units + Rooms + Beds + Baths + SaleYr, 
          data = sp) 

summary(reg)
```

```{r eval=FALSE, include=FALSE}
#read(transport) jz
transport <- read.csv("D:/GIS/mid term/midtermData_studentVersion/Muni_Stops.csv")
transport.sf <- 
   transport %>% 
   st_as_sf(coords = c("LONGITUDE", "LATITUDE"), crs = 4326, agr = "constant") %>%
   st_transform(2227)
```

```{r eval=FALSE, include=FALSE}
#download the data from ACS 5 yr 2017

options(tigris_use_cache = TRUE)
#census_api_key("da0e988e55392485b75bc3cc569605b96273839a", install = TRUE)
#readRenviron("~/.Renviron")
#Sys.getenv("CENSUS_API_KEY")

#load acs profile and detailed tables
acs17profile <- load_variables(2017, "acs5/profile", cache = FALSE)
acs17 <- load_variables(2017, "acs5", cache = FALSE)

#below are the data from ACS profile
tracts17_p <- 
  get_acs(geography = "tract", variables = c("DP02_0067P", "DP05_0019P","DP05_0024P", 
                                             "DP02_0101P", "DP02_0079P", "DP04_0003P", 
                                             "DP04_0007P", "DP04_0047P", "DP04_0060P",
                                             "DP04_0061P", "DP04_0089", "DP04_0134"), 
          year = 2017, state=06, county=075, geometry=T) %>%
  st_transform(2227)  %>%
  dplyr::select(variable, estimate, GEOID) %>%
  spread(variable, estimate) %>%
  rename(pcbachedegree = DP02_0067P,
         pcunder18 = DP05_0019P,
         pcabove65 = DP05_0024P,
         pcforeighborn = DP02_0101P,
         pcsamehouse = DP02_0079P,
         pcvacant = DP04_0003P,
         pcdetached = DP04_0007P,
         pcrenter = DP04_0047P,
         pc2vehicles = DP04_0060P,
         pc3ormorevehicles = DP04_0061P,
         medianhomevalue = DP04_0089,
         medianrent = DP04_0134)

#below are the data from ACS detailed table
tracts17 <- 
  get_acs(geography = "tract", variables = c("B02001_002","B02001_003","B02001_005","B03002_012",
                                             "B19013_001", "B01003_001"), 
          year = 2017, state=06, county=075, geometry=T) %>%
  st_transform(2227)  %>%
  dplyr::select(variable, estimate, GEOID) %>%
  spread(variable, estimate) %>%
  rename(white = B02001_002,
         black = B02001_003,
         asian = B02001_005,
         hispanic = B03002_012,
         med_hhincome = B19013_001,
         totalpop = B01003_001) %>%
  mutate(pcWhite = white / totalpop,
         pcBlack = black / totalpop,
         pcAsian = asian / totalpop,
         pcHispanic = hispanic / totalpop)

tracts17 = st_drop_geometry(tracts17)
tracts17tot <- merge(tracts17, tracts17_p, by = "GEOID")
```

```{r eval=FALSE, include=FALSE}
#read(park) JZ

park <- st_read("https://data.sfgov.org/api/geospatial/gtr9-ntp6?method=export&format=GeoJSON")
park$latitude <- as.numeric(as.character(park$latitude)) 
park$longitude <- as.numeric(as.character(park$longitude))
park.sf <-
  park %>%
    st_as_sf(coords = c("latitude", "longitude"), crs = 4326, agr = "constant") %>%
    st_transform(2227)

#add parkfilter
parkfilter <-
  park %>% 
  filter(latitude > 37.7, longitude < -122)

parkfilter.sf <-
  park %>% 
  filter(latitude > 37.7, longitude < -122)

parkfilter_d.sf <-
  parkfilter %>%
    dplyr::select(latitude, longitude) %>%
    na.omit() %>%
    st_as_sf(coords = c("Longitute", "Latitude"), crs = 4326, agr = "constant") %>%
    st_transform(2227) %>%
    distinct()

sp_original$park.Buffer =
    sp_original%>%
    st_buffer(2640) %>%
    aggregate(mutate(parkfilter_d.sf, park.Buffer = 1),., sum) %>%
    pull(park.Buffer)

park_cent <- st_centroid(parkfilter_d.sf)

sp_original$park_nn1 =
    nn_function(st_coordinates(sp_original), st_coordinates(park_cent), 1)

sp_original$park_nn2 =
    nn_function(st_coordinates(sp_original), st_coordinates(park_cent), 2)

sp_original$park_nn3 =
    nn_function(st_coordinates(sp_original), st_coordinates(park_cent), 3)

#park larger than 50 acres
parkfilter.sf$acres <- as.numeric(as.character(parkfilter.sf$acres)) 

park50.sf <- 
  parkfilter.sf %>% 
  select(acres) %>%
  filter(acres > 50)
park50.sf <- park50.sf[,2]
```

```{r eval=FALSE, include=FALSE}
#(school district) JZ
school_districts <- 
  st_read("https://data.sfgov.org/api/geospatial/tpp3-epx2?method=export&format=GeoJSON") %>% 
  st_transform(2227) 

#school distance nn
school_district.sf <-
  school_districts %>%
    dplyr::select(geometry) %>%
    na.omit() %>%
    st_transform(2227) %>%
    distinct()

sp_original$school.buffer =
    sp_original%>%
    st_buffer(2640) %>%
    aggregate(mutate(school_district.sf, school.Buffer = 1),., sum) %>%
    pull(school.Buffer)

sp_original$school_nn1 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 1)

sp_original$school_nn2 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 2)

sp_original$school_nn3 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 3)

sp_original$school_nn4 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 4)

sp_original$school_nn5 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 5)

school_districtsprivate <- subset(school_districts, ccsf_entity == "Private")

#private school distance
school_districtsprivate.sf <-
  school_districtsprivate %>%
    dplyr::select(geometry) %>%
    na.omit() %>%
    st_transform(2227) %>%
    distinct()

sp_original$school.bufferpr =
    sp_original%>%
    st_buffer(2640) %>%
    aggregate(mutate(school_districtsprivate.sf, school.bufferpr = 1),., sum) %>%
    pull(school.bufferpr)

sp_original$schoolp_nn1 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 1)

sp_original$schoolp_nn2 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 2)

sp_original$schoolp_nn3 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 3)

sp_original$schoolp_nn4 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 4)

sp_original$schoolp_nn5 =
    nn_function(st_coordinates(sp_original), st_coordinates(school_district.sf), 5)

```


```{r eval=FALSE, include=FALSE}
#slope hy
slope <- 
  st_read("https://data.sfgov.org/api/geospatial/rnbg-2qxw?method=export&format=GeoJSON") %>%
  st_set_crs(4326) %>%
  st_transform(2227)
```

```{r eval=FALSE, include=FALSE}
#commercial area and open space, JZ
#distance to nearest commercial area and open space&rec
distance_op_co <- 
  st_read("D:/GIS/mid term/midtermData_studentVersion/midtermData_studentVersion/Land Use/Distance_Op_Co_et.shp") %>%
  st_transform(2227) %>% 
  filter(SalePrice!=0)

distance_op_co.sf <- 
  distance_op_co %>% 
  st_transform(2227)

sp_original <-
  sp_original %>%
    st_join(dplyr::select(distance_op_co,Distance,Distance_1))

sp_original %>%
  dplyr::select(Distance, SalePrice) %>%
  st_set_geometry(NULL) %>%
  gather(Variable, Value, -SalePrice) %>%
    ggplot(aes(Value, SalePrice)) +
      geom_point() +
      geom_smooth(method = "lm", se=F, colour = "#25CB10")
```

```{r eval=FALSE, include=FALSE}
#crime wjz
crime.sf <- 
  st_read("C:/Users/Jingzong/Google Drive/MUSA/Mid_Project/HomePricePrediction/CRIME.geojson")%>% 
  st_transform(2227)
```
```{r eval=FALSE, include=FALSE}
ggplot() +
  geom_sf(data=bsmp)+
  scale_colour_manual(values = palette5,
                   labels=qBr(sp,"SalePrice"),
                   name="Quintile\nBreaks") +
  geom_sf(data = crime.sf, colour = "black", size = .1) +
  geom_sf(data=sp，aes(colour = q5(SalePrice)), show.legend = "point", size = .1)+
  labs(title = "2012-2015 aggravated assaults&roberry, SF") +
  mapTheme()
```

```{r eval=FALSE, include=FALSE}
#crime ready
sp_original1$crime_nn1 =
    nn_function(st_coordinates(sp_original1), st_coordinates(crime.sf), 1)
sp_original1$crime_nn2 =
    nn_function(st_coordinates(sp_original1), st_coordinates(crime.sf), 2)
sp_original1$crime_nn3 =
    nn_function(st_coordinates(sp_original1), st_coordinates(crime.sf), 3)
sp_original1$crime_nn4 =
    nn_function(st_coordinates(sp_original1), st_coordinates(crime.sf), 4)
sp_original1$crime_nn5 =
    nn_function(st_coordinates(sp_original1), st_coordinates(crime.sf), 5)
```

```{r eval=FALSE, include=FALSE}
####elevation
elev_index <- 
  sp_original %>% 
  st_nearest_feature(dplyr::select(slope, elevation))
st_geometry(slope) <- NULL
```

```{r eval=FALSE, include=FALSE}
sp_elev <- data.frame()
i <- 1
while (i <= length(elev_index)) {
    i_elev <- as.data.frame(slope[elev_index[i],1])
    sp_elev <- rbind(sp_elev, i_elev)
  i <- i+1
}
```
```{r eval=FALSE, include=FALSE}
sp_original1$elevation <-  sp_elev$`slope[elev_index[i], 1]`
sp_original1$elevation <- as.numeric(levels(sp_original1$elevation))[sp_original1$elevation]
```

```{r eval=FALSE, include=FALSE}
as.data.frame(sp_original)  %>% 
filter(SalePrice!=0) %>% 
ggplot(aes(elevation, SalePrice)) +
     geom_point() +
      geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     theme(axis.text.x = element_blank()) +
     plotTheme()
```

```{r eval=FALSE, include=FALSE}
reg_ele <- 
  lm(SalePrice ~ elevation, data=sp_original)

summary(reg_ele)
```

```{r eval=FALSE, include=FALSE}
###slope slope wjz
slope <- read.csv("C:/Users/Jingzong/Google Drive/MUSA/Mid_Project/GIS/slope.txt")
```

```{r eval=FALSE, include=FALSE}
sp_original1 <- merge(sp_original1,slope,by.x=c("X", "Y"), by.y=c("X", "Y"))
colnames(sp_original1)[colnames(sp_original1)=="RASTERVALU"] <- "slope"
```

```{r eval=FALSE, include=FALSE}
reg_slop <- 
  lm(SalePrice ~ slope, data=sp_original)
summary(reg_slop)
```
```{r eval=FALSE, include=FALSE}
as.data.frame(sp_original)  %>% 
filter(SalePrice!=0) %>% 
ggplot(aes(slope, SalePrice)) +
     geom_point() +
      geom_smooth(method = "lm", se=F, colour = "#FA7800") +
     theme(axis.text.x = element_blank()) +
     plotTheme()
```

```{r eval=FALSE, include=FALSE}
#TREE CANOPY BY JZ
tree_canopy1 <- 
  st_read("D:/GIS/mid term/midtermData_studentVersion/midtermData_studentVersion/midterm_data_sf_tree_total2.shp") %>%
  st_transform(2227)

tree_canopy1 <- 
  as.data.frame(tree_canopy1) %>% 
  group_by(ParcelID) %>% 
  summarize(totaltree = sum(t_sqft))

sp_original <- merge(as.data.frame(tree_canopy1),as.data.frame(sp_original),by="ParcelID")

```

```{r eval=FALSE, include=FALSE}
#water JZ
water <- 
  st_read("D:/GIS/mid term/midtermData_studentVersion/Musa_Mid/Water bodies/water_distance.shp") %>% 
  st_transform(2227)

water <-
  water %>% 
  select(ParcelID,water)

water$geometry = NULL

sp_original1 <- merge(as.data.frame(water),as.data.frame(sp_original1),by="ParcelID")

sp_original1 %>%
  dplyr::select(water, SalePrice) %>%
  gather(Variable, Value, -SalePrice) %>%
    ggplot(aes(Value, SalePrice)) +
      geom_point() +
      geom_smooth(method = "lm", se=F, colour = "#25CB10") +
      facet_wrap(~Variable)
```
 

```{r eval=FALSE, include=FALSE}
#HAZARDS JZ
hazards <- 
  st_read("D:/GIS/mid term/midtermData_studentVersion/Musa_Mid/San Francisco Seismic Hazard Zones/Join_Outputdistance2.shp") %>% 
  st_transform(2227)

hazards <-
  hazards %>% 
  select(ParcelID,Distance_N)

hazards$geometry = NULL

sp_original1 <- merge(as.data.frame(hazards),as.data.frame(sp_original1),by="ParcelID")


sp_original1 %>%
  dplyr::select(Distance.y, SalePrice) %>%
  gather(Variable, Value, -SalePrice) %>%
    ggplot(aes(Value, SalePrice)) +
      geom_point() +
      geom_smooth(method = "lm", se=F, colour = "#25CB10") +
      facet_wrap(~Variable)
```

```{r eval=FALSE, include=FALSE}
#census data cleaning by hy, cleaned data in dataframe sp_original1
#spatial join census to the home price
tracts17tot.sf <- st_as_sf(tracts17tot)
sp_original1 <- st_join(sp_original, tracts17tot.sf)

#transform builtyear from factors to numbers
sp_original1$BuiltYear <- as.numeric(levels(sp_original1$BuiltYear))[sp_original1$BuiltYear]

#transform builtyear to age
sp_original1 <- sp_original1 %>%
  mutate(houseAge = 2019 - BuiltYear)

#data cleaning: assign correct stories of the houses
sp_original1$Stories[sp_original1$Stories==829] <- 2 #parcel id 1242021
sp_original1$Stories[sp_original1$Stories==742] <- 1 #7016023
sp_original1$Stories[sp_original1$Stories==432] <- 2 #1153051
sp_original1$Stories[sp_original1$Stories==10] <- 2 #6655075
#data cleaning: Rooms
sp_original1$Rooms[sp_original1$Rooms==1353] <- 6 #4148041 used median as substitute
#data cleaning: Baths
sp_original1$Baths[sp_original1$Baths==25] <- 2 #7106045A,6481018 used median as substitute
#data cleaning: PropArea
sp_original1$PropArea[sp_original1$PropArea==24308] <- 2500 #5837002 used max neighbor area as substitute
sp_original1$PropArea[sp_original1$PropArea==22650] <- 3750 #3621070 
#data cleaning: Beds
sp_original1$Beds[sp_original1$Beds==20] <- 2 #0552062 used neighbor data as substitute
#rooms = 0 still need to be cleaned
```

```{r eval=FALSE, include=FALSE}
reg.results <- 
  data.frame(Observed = sp$SalePrice,
             Predicted = reg$fitted.values)

ggplot() + 
  geom_point(data=reg.results, aes(Observed, Predicted)) +
  stat_smooth(data=reg.results, aes(Observed, Observed), method = "lm", se = FALSE, size = 1, colour="red") + 
  stat_smooth(data=reg.results, aes(Observed, Predicted), method = "lm", se = FALSE, size = 1, colour="blue") + 
  labs(title="Predicted Sales Price as a function\nof Observed Sales Price",
       subtitle="Perfect prediction in red; Actual prediction in blue") +
  theme(plot.title = element_text(size = 18,colour = "black"))
```

```{r eval=FALSE, include=FALSE}
hist(abs(sp$SalePrice - reg$fitted.values), breaks=50, main="Histrogram of residuals (absolute values)")
```

## 2.2 Exploratory Analysis
### 2.2.1 Summary Statistics

We have categorized our predictors into three types: internal characteristics, amenities/public services, and spatial structue. Their summary statistics are presented below. Note that a few variables are not in the tables below since they are spatial or categorical data, including the neighborhoods and zonings.

![](D:/GIS/mid term/New folder/HJZ.Group.Inc/plot/Capture1.PNG)

![](D:/GIS/mid term/New folder/HJZ.Group.Inc/plot/Capture2.PNG)

![](D:/GIS/mid term/New folder/HJZ.Group.Inc/plot/Capture3.PNG)

```{r eval=FALSE, include=FALSE}
#stargazer(as.data.frame(sp_original1), nobs = FALSE, type="text", title = "Summary Statistics")

stargazer(as.data.frame(sp_original1)[c("SalePrice", "LotArea", "PropArea", "BuiltYear", "Stories", "Rooms", "Beds", "Baths", "SaleYr", "houseAge")], digits = 2, type = "text", nobs = FALSE, title = "Internal Characteristics")

stargazer(as.data.frame(sp_original1)[c("Distance_N", "water", "Distance.x", "totaltree", "crime_nn1", "crime_nn2", "crime_nn3", "crime_nn4", "crime_nn5", "park.Buffer", "park_nn1", "park_nn2", "park_nn3", "school.buffer", "school_nn1", "school_nn2", "school_nn3", "school_nn4", "school_nn5", "Distance.y", "transit.buffer", "tech_nn3", "tech_nn4", "tech_nn5", "art_nn3", "art_nn4", "art_nn5", "real_est_nn3", "real_est_nn4", "real_est_nn5", "Pr_hlth_nn3", "Pr_hlth_nn4", "Pr_hlth_nn5", "Financ_nn3", "Financ_nn4", "Financ_nn5","ind_nn1", "ind_nn2", "ind_nn3", "min_dis_road", "Distance")], digits = 2, type = "text", nobs = FALSE, title = "Amenities/Public Services")

stargazer(as.data.frame(sp_original1)[c("totalpop", "med_hhincome", "pcWhite", "pcBlack", "pcAsian", "pcHispanic","pcbachedegree", "pcsamehouse", "pcforeighborn", "pcvacant", "pcdetached", "pcrenter", "pc2vehicles", "pc3ormorevehicles", "medianhomevalue", "medianrent", "pcunder18", "pcabove65", "elevation", "slope", "lagPrice")], digits = 2, type = "text", nobs = FALSE, title = "Spatial Structure")
```

### 2.2.2 Correlation between all the variables

A correlation matrix is produced to see the relationship between the continuous variables. The non-continuous variables, including all the census data variables, are dropped. To further simplify the correlation plot, only one variable is kept for each amenity. 

![](D:/GIS/mid term/New folder/HJZ.Group.Inc/plot/unnamed-chunk-38-1.png)

```{r eval=FALSE, include=FALSE}
sp_original1cor <- 
  filter(sp_original1, holdOut =="0") %>%
  st_set_geometry(NULL) %>%
  dplyr::select(SalePrice, LotArea, PropArea, Stories, Rooms, Beds, Baths, SaleYr, houseAge, Distance_N, water, Distance.x, totaltree, crime_nn5, park_nn3, school_nn5, Distance.y, transit.buffer, tech_nn5,art_nn5,real_est_nn5,Pr_hlth_nn5, Financ_nn5, ind_nn3, min_dis_road, Distance, elevation, lagPrice)

#totalpop, med_hhincome, pcWhite, pcBlack, pcAsian, pcHispanic, pcbachedegree, pcsamehouse, pcforeighborn, pcvacant, pcdetached, pcrenter, pc2vehicles, pc3ormorevehicles, medianhomevalue, medianrent, pcunder18, pcabove65,

M <- cor(sp_original1cor)
corrplot(M, number.cex = .7, tl.srt = 45, method = "color", type = "lower", tl.col = "black", na.label = ".", na.rm = T, tl.cex = 0.7, col = brewer.pal(n = 10, name = "RdYlBu"))
```

A series explorations of correlation between the dependent variable sale price and other predictors are conducted using the scatter plots. Here, four of the most well-fitted scatter plots are presented. `Fianc_nn5` is the distance to 5 Nearest financial services, `ind_nn3` is the distance to 3 nearest attractions, `pcsamehouse` is the percentage of residents lived in the same house for over 1 year, and `tech_nn5` is the distance to 5 nearest tech companies.

![](D:/GIS/mid term/New folder/HJZ.Group.Inc/plot/unnamed-chunk-39-1.png)

```{r eval=FALSE, include=FALSE}
labeli <- function(variable, value){
  names_li <- list("Financ_nn5"="Distance to 5 Nearest Financial Services", "ind_nn3"="Distance to 3 Nearest Attractions", "pcsamehouse"="Percentage of Residents Lived in the Same House for over 1 Year", "tech_nn5"="Distance to 5 Nearest Tech Companies")
  return(names_li[value])
}

as.data.frame(sp_original1) %>% 
  dplyr::select(SalePrice,  Financ_nn5, ind_nn3, pcsamehouse, tech_nn5) %>%
  filter(SalePrice != 0) %>%
  gather(Variable, Value, -SalePrice) %>% 
  ggplot(aes(Value, SalePrice), labeller = labeli) +
  geom_point() +
  geom_smooth(method = "lm", se=F, colour = "#FA7800") +
  facet_wrap(~Variable, ncol = 2, scales = "free", ) +
  labs(title = "Price as a Function of Four Predictors") +
  plotTheme()
```

### 2.2.3 Spatial Distribution

Three of the independent variables are mapped below. For the first two variables, both the density of the amenity and the distance to the 5 nearest amenities are mapped. For the third variable, elevation, both the point elevation and the countour lines are mapped.

![](D:/GIS/mid term/New folder/HJZ.Group.Inc/plot/unnamed-chunk-42-1.png)

![](D:/GIS/mid term/New folder/HJZ.Group.Inc/plot/unnamed-chunk-44-1.png)

![](D:/GIS/mid term/New folder/HJZ.Group.Inc/plot/unnamed-chunk-47-1.png)

# 3.Feature Engineering and Selection

In this project, we are using Ordinary Least Square (OLS) as the model to estimate house prices. OLS is able to find the linear relationship between the dependent variable, which in this case is the house sale price, and the predictors, which are the variables mentioned above in the data collection section. The feature engineering is done in the following steps.

1. In order to construct and choose the most effective features, we ran an OLS linear regression model for all the predictors to test the fitness of the model. The output gives both the p-value and the coefficient for each variable. The p-value reflects the significance of the coefficient, in other words, the probability of having the coefficient assuming there is no correlation between the independent and the dependent variables. Typically, we want p-value to be smaller than 0.05. R-squared is also calculated, which demonstrates the amount of variance in the dependent variable that is explained by the predictors. A larger R-squared means a better estimation of the model and is desired.

2. We compare the p-values of the variables to check their significances in relation to the house price. This gives us a direction of what variables to keep or change. After removing or adding variables, we ran OLS model again to compare the R-squared value. Multiple trials are done to select the best set of predictors.

3. A series of feature engineering is done throughout the trial process. We have log-transformed the price related to the predictors in order to make them better fit for the linear model. We also have made several variables into categories, since continuous variables do not always have the most predictive power. For example, in a demand perspective, a potential mansion purchaser would not care about the exact numbers of the rooms, therefore, a house with 6 or 7 bedrooms might not influence the final house price that much.

4. We also have used the stepwise regression using the `stepAIC` function in the `MASS` package, which automatically finds the best set of predictors with the least R-squared value. However, since we have more model validation in the next step, we only used this function result as a reference.


```{r eval=FALSE, include=FALSE}
county <- 
  st_read("https://data.sfgov.org/api/geospatial/s9wg-vcph?method=export&format=GeoJSON") %>%
  st_set_crs(4326) %>%
  st_transform(2227)

countysf <- subset(county, county == "San Francisco")

countysf <- 
  countysf  %>% 
  st_transform(st_crs(nhood)) %>%
  st_intersection(.,nhood)
```

```{r eval=FALSE, include=FALSE}
real_est1 <- 
  real_est  %>% 
  st_intersection(.,nhood)
```

```{r eval=FALSE, include=FALSE}
ggplot() + 
  geom_sf(data = sf_county, fill = "grey90") +
  stat_density2d(data = data.frame(st_coordinates(real_est1)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "#fee391", high = "#cc4c02", name = "Density of Real Estates \nand Leasing Services") +
  scale_alpha(range = c(.1, .3), guide = FALSE) +
  geom_sf(data = sp_original1, aes(color = real_est_nn5), show.legend = "point", size = .75) + 
    labs(title="Real Estate and Leasing Services Proximity") +
    scale_colour_viridis(name="House Distance to \n5 Closest Real Estate\nand Leasing Services, ft") +
  mapTheme()
```

```{r eval=FALSE, include=FALSE}
school_district.sf1 <- 
  school_district.sf  %>% 
  st_transform(st_crs(nhood)) %>%
  st_intersection(.,nhood)
```

```{r eval=FALSE, include=FALSE}
ggplot() + 
  geom_sf(data = sf_county, fill = "grey90") +
  stat_density2d(data = data.frame(st_coordinates(school_district.sf1)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_gradient(low = "#fee391", high = "#cc4c02", name = "Density of Schools") +
  scale_alpha(range = c(0, .2), guide = FALSE) +
  geom_sf(data = sp_original1, aes(color = school_nn5), show.legend = "point", size = .75) + 
    labs(title="School Proximity, SF") +
    scale_colour_viridis(name="House Distance to the\n5 Closest Schools\nft") +
  mapTheme()
```

```{r eval=FALSE, include=FALSE}
slope1 <- 
  st_read("https://data.sfgov.org/api/geospatial/rnbg-2qxw?method=export&format=GeoJSON") %>%
  st_set_crs(4326) %>%
  st_transform(2227) %>%
  st_intersection(.,nhood)
```
```{r eval=FALSE, include=FALSE}
slope1$elevation <- as.numeric(levels(slope1$elevation))[slope1$elevation]
slope1 <- slope1[ which(slope1$elevation==-50 | slope1$elevation==50 | slope1$elevation==150| slope1$elevation==250|slope1$elevation==350|slope1$elevation==450|slope1$elevation==550|slope1$elevation==650|slope1$elevation==750|slope1$elevation==850|slope1$elevation==950), ]
```


```{r eval=FALSE, include=FALSE}
ggplot() + 
  geom_sf(data = sf_county, fill = "grey90") +
  geom_sf(data = slope1, color = "grey70", size = 0.1) +
  geom_sf(data = sp_original1, aes(color = elevation), show.legend = "point", size = .75) + 
    labs(title="Elevation") +
    scale_colour_viridis(name="Elevation, ft") +
  mapTheme()
```




```{r eval=FALSE, include=FALSE}
sp_model <- filter(sp_original1,holdOut == "0")
sp_model.sf <- st_as_sf(sp_model)
inTrain <- createDataPartition(
              y = paste(sp_model.sf$zoning_sim,sp_model.sf$nbrhood,sp_model.sf$gen_hght,sp_model.sf$PropClassC), 
              p = .60, list = FALSE)
sp.training <- sp_model.sf[inTrain,] 
sp.test <- sp_model.sf[-inTrain,] 
```


```{r eval=FALSE, include=FALSE}
reg1 <- lm(SalePrice_ln ~ PropClassC + LotArea + PropArea + Stories +   
          Rooms_cat + Beds_cat + Baths + SaleYr + crime_nn5 + school_nn3 + park_nn1 + Distance_N + Distance.y + totaltree + transit_cat + hght_cat + elev.cat1 + zoning_sim  + med_hhincome_ln + pcWhite + pcBlack + pcAsian + pcHispanic + pcbachedegree + houseAge + pcunder18 + pcabove65 + pc2vehicles + totalpop + pc3ormorevehicles + pcrenter + pcdetached + pcvacant + pcsamehouse + pcdetached + medianhomevalue_ln + medianrent_ln + water  + Pr_hlth_nn5 + Financ_nn5 + ind_nn3 + nbrhood + lagPrice_ln,
          data = sp.training) 

summary(reg1)

```

```{r eval=FALSE, include=FALSE}
sp.test <-
  sp.test %>%
  mutate(SalePrice.Predict = predict(reg1, sp.test),
         SalePrice.Predict = exp(SalePrice.Predict),
         SalePrice.Error = SalePrice - SalePrice.Predict,
         SalePrice.AbsError = abs(SalePrice - SalePrice.Predict),
         SalePrice.APE = (abs(SalePrice - SalePrice.Predict) / SalePrice))
```

```{r eval=FALSE, include=FALSE}
st_set_geometry(sp.test, NULL) %>%
  summarize(mean(SalePrice.AbsError, na.rm = T)) %>%
  pull()

st_set_geometry(sp.test, NULL) %>%  
  summarize(mean(SalePrice.APE, na.rm = T) * 100) %>%
  round(2) %>%
  paste("%")
```


```{r eval=FALSE, include=FALSE}
x <- c(1:10)
sum_MAE <- 0
sum_MAPE <- 0
for (val in x){
  sp_model <- filter(sp_original1,holdOut == "0")
sp_model.sf <- st_as_sf(sp_model)
inTrain <- createDataPartition(
              y = paste(sp_model.sf$zoning_sim,sp_model.sf$nbrhood,sp_model.sf$gen_hght,sp_model.sf$PropClassC), 
              p = .60, list = FALSE)
sp.training <- sp_model.sf[inTrain,] 
sp.test <- sp_model.sf[-inTrain,]

reg1 <- lm(SalePrice_ln ~ PropClassC + LotArea + PropArea + Stories +   
          Rooms_cat + Beds_cat + baths_cat + SaleYr + crime_nn5 + Distance_N_cat + Distance_cat + elev.cat1 + zoning_sim  + med_hhincome_ln + white_cat + pcBlack + pcAsian + pcHispanic + pcbachedegree + houseAge + pcunder18 + pcabove65 + pc2vehicles + totalpop + pc3ormorevehicles + pcrenter + pcdetached + pcvacant + pcsamehouse + pcdetached + medianhomevalue_ln + medianrent_ln + water_cat + Pr_hlth_nn5 + Financ_nn5 + ind_nn3 + nbrhood + lagPrice_ln + sdate + school_nn3 + totoltree_cat + transit_cat + park_nn2,
          data = sp.training)

sp.test <-
  sp.test %>%
  mutate(SalePrice.Predict = predict(reg1, sp.test),
         SalePrice.Predict = exp(SalePrice.Predict),
         SalePrice.Error = SalePrice - SalePrice.Predict,
         SalePrice.AbsError = abs(SalePrice - SalePrice.Predict),
         SalePrice.APE = (abs(SalePrice - SalePrice.Predict) / SalePrice))

MAE_i <- st_set_geometry(sp.test, NULL) %>%
  summarize(mean(SalePrice.AbsError, na.rm = T))
sum_MAE <- MAE_i+sum_MAE

MAPE_i <- st_set_geometry(sp.test, NULL) %>%  
  summarize(mean(SalePrice.APE, na.rm = T) * 100) %>%
  round(4)
sum_MAPE <- MAPE_i+sum_MAPE
}
mean_MAE <- sum_MAE/10
mean_MAPE <- sum_MAPE/10
mean_MAE
mean_MAPE
```

```{r eval=FALSE, include=FALSE}
ggplot(filter(sp.test,SalePrice.AbsError <= 1000000), aes(SalePrice.AbsError)) +
  geom_histogram(bins = 100, colour = "white", fill="#25CB10") +
  scale_x_continuous(breaks = seq(0, 1000000, by = 100000)) +
  labs(title="Distribution of prediction errors") +
  plotTheme()
```

```{r eval=FALSE, include=FALSE}
as.data.frame(sp.test) %>%
  dplyr::select(SalePrice, SalePrice.Predict) %>%
  gather(Variable, Value) %>%
  ggplot(aes(Value, fill = Variable)) + 
    geom_density(alpha = 0.5) + 
    scale_fill_manual(values = c("#25CB10", "#FA7800")) +
    labs(title="Distribution of sale price & sale price predictions", 
         x = "Price/Prediction", y = "Density of observations") +
    plotTheme()
```

```{r eval=FALSE, include=FALSE}
ggplot(sp.test, aes(SalePrice.Predict, SalePrice)) + 
  geom_point() +
  stat_smooth(data=sp.test, aes(SalePrice, SalePrice), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  stat_smooth(data=sp.test, aes(SalePrice, SalePrice.Predict), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  labs(title="Predicted sale price as a function of\nobserved price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  theme(plot.title = element_text(size = 18,colour = "black"))
```

```{r eval=FALSE, include=FALSE}
fitControl <- trainControl(method = "cv", number = 100)

set.seed(825)

reg4.cv <- train(SalePrice_ln ~ ., data = as.data.frame(sp_model.sf) %>% 
                                               dplyr::select(SalePrice_ln,PropClassC, LotArea, PropArea, Stories, Rooms_cat, Beds_cat, Baths,SaleYr,crime_nn5,school_nn2,park_nn2,Distance_N,Distance.y,totaltree,transit_cat,elev.cat1,slope_cat,med_hhincome_ln,pcWhite,pcBlack,pcAsian,pcHispanic,pcbachedegree,houseAge,pcunder18,pcabove65,pc2vehicles,totalpop,pc3ormorevehicles,pcrenter,pcdetached,pcvacant,pcsamehouse,pcdetached,medianhomevalue,medianrent,water,tech_nn3,Financ_nn5,ind_nn1,nbrhood,lagPrice), 
                 method = "lm", trControl = fitControl, na.action = na.pass)
reg4.cv

reg4.cv$results


ggplot(as.data.frame(reg4.cv$resample), aes(Rsquared)) + 
  geom_histogram(bins = 50, colour="white", fill = "#FA7800") +
  labs(title="Distribution of MAE", subtitle = "k-fold cross validation; k = 100",
       x="Mean Absolute Error", y="Count") +
  xlim(0,1) +
  plotTheme()

ggplot(as.data.frame(reg4.cv$resample), aes(MAE)) + 
  geom_histogram(bins = 50, colour="white", fill = "#FA7800") +
  labs(title="Distribution of MAE", subtitle = "k-fold cross validation; k = 100",
       x="Mean Absolute Error", y="Count") +
  plotTheme()
```


```{r eval=FALSE, include=FALSE}
#isolate holdout ==1
sp_holdout0 <- subset(sp_original, holdOut == 0)
sp_holdout0 = subset(sp_holdout0, select = c(SalePrice,geometry) )
#house buffer
sp_original1$SalePrice.buffv =
  sp_original1 %>%
  st_buffer(300) %>%
  aggregate(mutate(sp_holdout0, SalePrice.buffv = SalePrice),., mean) %>%
  #aggregate(mutate(sp_holdout0, ), mean) %>%
  #aggregate(mutate(sp_holdout0, SalePrice.buff = 1),., sum) %>%
  pull(SalePrice.buffv)

sp_original1$SalePrice.buff =
  sp_original1 %>%
  st_buffer(300) %>%
  #aggregate(mutate(sp_holdout0, SalePrice.buff = SalePrice),., mean) %>%
  #aggregate(mutate(sp_holdout0, ), mean) %>%
  aggregate(mutate(sp_holdout0, SalePrice.buff = 1),., sum) %>%
  pull(SalePrice.buff)

sp_original1$SalePrice.buffv_ln = log(sp_original1$SalePrice.buffv)
```


```{r eval=FALSE, include=FALSE}
#plot sale price and sale price absolute errors spatially
sp_original1 <- st_as_sf(sp_original1)
grid.arrange(
  ggplot() +
    geom_sf(data = nhood, fill = "grey40") +
    geom_sf(data = sp_original1, aes(colour = q5(SalePrice)), show.legend = "point", size = 1) +
    scale_colour_manual(values = palette5,
                        labels=qBr(sp_original1,"SalePrice"),
                        name="Quintile\nBreaks") +
    labs(title="Test set sale prices") +
    mapTheme(),
  
  ggplot() +
    geom_sf(data = nhood, fill = "grey40") +
    geom_sf(data = sp_original1, aes(colour = q5(SalePrice.AbsError)), show.legend = "point", size = 1) +
    scale_colour_manual(values = palette5,
                        labels=qBr(sp_original1,"SalePrice.AbsError"),
                        name="Quintile\nBreaks") +
    labs(title="Sale price errors on the test set") +
    mapTheme(),
  ncol = 2)

#spatial lag
coords <- st_coordinates(sp_original1) 
neighborList <- knn2nb(knearneigh(coords, 5))
spatialWeights <- nb2listw(neighborList, style="W")

sp_original1$lagPrice <- lag.listw(spatialWeights, sp_original1$SalePrice)

ggplot(sp_original1, aes(lagPrice, SalePrice)) +
  geom_point(colour = "#FA7800") +
  geom_smooth(method = "lm", se = FALSE, colour = "#25CB10") +
  labs(title = "Price as a function of the spatial lag of price",
       subtitle = "As a measure of spatial autocorrelation; lag of 5 nearest neighbors",
       caption = "put caption here",
       x = "Spatial lag of price (Mean price of 5 nearest neighbors)",
       y = "Sale Price") +
  plotTheme()
```

# 4.Model Estimation and Validation
## 4.1 Methods

It is critical that models generalize to data they haven’t seen before. The R^2 measures estimated measure error based on the data from which the model was trained. Below, data is split into training and test datasets. Models will be trained on the former and tested on the latter.

As we mentioned above, the total dataset is divided into `training` and `test` part, which represent 60% and 40% respectively. The table below discribes in-sample (training set) model results. We inlude all the variables in the training set, so that it is clear to see which variable has more significant influence. The estimate illustrates the coiefficient, the next three columns are standard error, t-value and p-value. By scrolling the table, it is easier to compare these variables. 
```{r}  
kable(model_name) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),full_width = F) %>% 
  row_spec(0, bold = T, color = "white", background = "#636363") %>% 
  column_spec(1, bold = T, color = "black", background = "#bdbdbd") %>%
  column_spec(2, bold = T, color = "black", background = "#deebf7") %>% 
  column_spec(3, bold = T, color = "black", background = "#e5f5e0") %>% 
  column_spec(4, bold = T, color = "black", background = "#deebf7") %>% 
  column_spec(5, bold = T, color = "black", background = "#e5f5e0") %>% 
  scroll_box(width = "100%", height = "200px")
```

The summary of r-squared and adjusted r-squared for the training sample are also demonstrated below. Generally, 85% of the variance of the dependent variable are explained in this prediction. 
```{r}
kable(model_names) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),full_width = F) %>% 
  row_spec(1, bold = T, color = "white", background = "green")
  
```

As we analysis the test set which occupied 40% of the total set, the prediction model still has some inacurracy. The graphic below shows the saleprice compared with saleprice predict. 
```{r}
as.data.frame(sp.test) %>%
  dplyr::select(SalePrice, SalePrice.Predict) %>%
  gather(Variable, Value) %>%
  ggplot(aes(Value, fill = Variable)) + 
    geom_density(alpha = 0.5) + 
    scale_fill_manual(values = c("#25CB10", "#FA7800")) +
    labs(title="Distribution of sale price & sale price predictions", 
         x = "Price/Prediction", y = "Density of observations") +
    plotTheme()
```
 
Generally, in this test set, the mean absError is almost 180000, while the mean absError percent is about 16%. 
```{r}
kable(sp.test_table) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),full_width = F) %>% 
  row_spec(1, bold = T, color = "white", background = "blue")
```

Another diagram which can illustrate the distribution of prediction errors in test set. The majority are within 0-200000 absError range, while some of them are beyond 400000. 
```{r}
ggplot(filter(sp.test,SalePrice.AbsError <= 1000000), aes(SalePrice.AbsError)) +
  geom_histogram(bins = 100, colour = "white", fill="#25CB10") +
  scale_x_continuous(breaks = seq(0, 1000000, by = 100000)) +
  labs(title="Distribution of prediction errors") +
  plotTheme()
```


Estimating a model on training set and predicting for test set is a good way to understand how well the model might generalize or predict for homes that haven’t actually sold. However, in order to improve generalizability, we perform cross-validation method, which will process the entire data 100 times. Cross-validation allows one to judge generalizability not on one random hold out but on many, helping to ensure that the goodness of fit on one hold out is not a fluke. 

![](D:/GIS/mid term/New folder/HJZ.Group.Inc/plot/1920px-K-fold_cross_validation_EN.svg.png)

Source:https://en.wikipedia.org/wiki/Cross-validation_(statistics)#K-fold_cross-validation

Below is the result of our cross-validation model.we didn't use the log price data in this model because it's hard to transform back to the actual price in Cross-Validation. In this case, the MAE is a littile bit high which is about 200000 compared with 180000 before. 

```{r}

reg4.cv$results

ggplot(as.data.frame(reg4.cv$resample), aes(MAE)) + 
  geom_histogram(bins = 50, colour="white", fill = "#FA7800") +
  labs(title="Distribution of MAE", subtitle = "k-fold cross validation; k = 100",
       x="Mean Absolute Error", y="Count") +
  plotTheme()
```

When we use the linear model to visualize the predicted and actual sale price, it is obvious that there are still some error existing. 
```{r}
ggplot(sp.test, aes(SalePrice.Predict, SalePrice)) + 
  geom_point() +
  stat_smooth(data=sp.test, aes(SalePrice, SalePrice), 
             method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  stat_smooth(data=sp.test, aes(SalePrice, SalePrice.Predict), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  labs(title="Predicted sale price as a function of\nobserved price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction") +
  theme(plot.title = element_text(size = 18,colour = "black"))
```

Two maps below illustrates the test set sale price and absolute sale price errors. It looks like more errors happened in center and northern part of San Francisco, which are the higher housing value areas. 
```{r}
  ggplot() +
    geom_sf(data = nhood, fill = "grey90") +
    geom_sf(data = sp.test, aes(colour = q5(SalePrice)), show.legend = "point", size = 0.75) +
    scale_colour_manual(values = palette5,
                     labels=qBr(sp.test,"SalePrice"),
                     name="Quintile\nBreaks") +
    labs(title="Test set sale prices") +
    mapTheme()
  
  ggplot() +
    geom_sf(data = nhood, fill = "grey90") +
    geom_sf(data = sp.test, aes(colour = q5(SalePrice.AbsError)), show.legend = "point", size = 0.75) +
    scale_colour_manual(values = palette5,
                     labels=qBr(sp.test,"SalePrice.AbsError"),
                     name="Quintile\nBreaks") +
    labs(title="Absolute sale price errors on the test set") +
    mapTheme()
```

The Moran’s I statistic is a statistical hypothesis test that asks whether and to what extent a spatial phenomenon exhibits a given spatial process.In practice, the test is looking to see how local means deviate from the global mean. A Moran’s I that is positive nearing the value of 1, describes positive spatial autocorrelation, also known as clustering. Instances where high and low prices “repel” one another, are said to be dispersed.  Finally, where positive and negative values are randonly distributed, the Moran’s I statistic is 0.

In our model, the Moran's I is 0.02 which is a good number. In other words, it means our model is randonly distributed. 
```{r}
#moran's I
moranTest <- moran.mc(filter(sp.test, !is.na(SalePrice.Error))$SalePrice.Error, 
                      spatialWeights.test , nsim = 999)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in red",
       x="Moran's I",
       y="Count",
       caption="Public Policy Analytics, Figure 6.8") +
  plotTheme()

moranTest
```

Another map provides the predicted values for the entire dataset
```{r}
ggplot()+
   geom_sf(data=nhood)+
   geom_sf(data=sp_model，aes(colour = q5(SalePrice.Predict)), show.legend = "point", size = .75)+
   scale_colour_manual(values = palette5,
                    labels=qBr(sp_model,"SalePrice.Predict"),
                    name="Quintile\nBreaks") +
   labs(title="Sale Price-Predict, SF") +
   mapTheme()
```
It is clear our model is consistent across the neighborhoods to some extend. We can conclude that our model is generalizable. However, more errors occur when we predict wealthier communities. 
```{r}
sptest.nhoods <-
  sp.test %>%
    group_by(nbrhood) %>%
    summarize(mean.MAPE = mean(SalePrice.APE, na.rm = T)) %>%
    na.omit(sptest.nhoods$nbrhood) %>% 
    st_set_geometry(NULL) %>%
    left_join(nhood) %>%
    st_sf()


ggplot() + 
   geom_sf(data = sptest.nhoods, aes(fill = q5(mean.MAPE))) +
   geom_sf(data = sp.test, colour = "black", size = .5) +
   scale_fill_manual(values = palette5,
                     labels = qBr(sptest.nhoods, "mean.MAPE", rnd = F),
                     name="Quintile\nBreaks") +
   labs(title = "Mean test set MAPE by neighborhood") +
   mapTheme()

```

```{r}
s <-
   sp.test %>% 
     group_by(nbrhood) %>% 
     summarize(mean.price = mean(SalePrice,na.rm=T)) %>% 
     na.omit(s$nbrhood) %>% 
    st_set_geometry(NULL) %>%
     left_join(nhood) %>%
     st_sf()

 
sptest.nhoods <- merge(as.data.frame(sptest.nhoods),as.data.frame(s),"nbrhood")

ggplot(sptest.nhoods, aes(mean.MAPE,mean.price)) + 
   geom_point() +
   stat_smooth(data=sptest.nhoods, aes(mean.MAPE,mean.price), 
              method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
   labs(title="MAPE by neighborhood as a function of mean price by neighborhood")+
   theme(plot.title = element_text(size = 10,colour = "black"))
```

Finally, when we divide income groups to test the generalizibilty, it has the same result as we disccused above. Higher income areas have lower prediction accuraccy.

![](D:/GIS/mid term/New folder/HJZ.Group.Inc/plot/income_context.png)

![](D:/GIS/mid term/New folder/HJZ.Group.Inc/plot/high-low_income.png)

## 4.2.   Discussion
The MAPE is about 16%, which means on average, our model only deviates from the actual price by 16%, which means it is pretty effective for house price estimations.

Feature engineering and selection is a very important step in our model construction. There are three types of features in our models:

1.Spatial Structure: One of the most important predictor features is the spatial lag variable, which calculated the mean prices of neighboring houses within a buffer distance of 1/16 miles for each house sale points. In this way, the model captures the spatial autocorrelation very well. We included this predictor in the model in the end and it significantly increases the model prediction.

2.Amenities: Other important variables include the distance to different types of services, including financial services and real estate leasing services. 

3.Internal Characteristics: Property area is the most significant factor that correlates with the home price.

The MAPE map has demonstrated whether the spatial autocorrelation is accounted for in our model. The darker blue regions where house prices higher tend to have higher MAPE, which means our model does not predict well for higher-priced homes values. In contrast, the model predicts the regions with lower house prices much better, with MAPE of about 7%, in other words, our prediction only deviates away from the true prices by 7%. We believe the spatial variation in MAPE is because the prices of high-value homes fluctuate more and are influenced by more complicated factors. 

# 5. Conclusion
We would highly recommend our model to Zillow, as it has a very small error in predicting house prices. Even though it is less representative in higher home value communities, the model still has great MAE, MAPE and r-squared. As we disccussed before, the model can be improved in multiple ways.Additionally, Non-linear models other OLS can be considered for better fit. 

